---
title: "jackie_lang_midterm"
output: 
  html_document:
    toc: TRUE
    toc_depth: 2
    self_contained: TRUE
    prettydoc::html_pretty:
    theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
warning = FALSE
message = FALSE
library(tidyverse)
```
#0 Github
##0.1
Link to my github repository: https://github.com/jrlangberkeley/compscix-415-2-assignments

#1 Tidyverse 
##1.1
a. ***ggplot2*** is the tidyverse package best tasked with plotting graphs. "ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details." (citation: http://ggplot2.tidyverse.org/)

b. ***dplyr*** is the tidyverse package for data munging and wrangling. It "is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges." (citation: http://dplyr.tidyverse.org/)

c. ***tidyr*** is the tidyverse package designed to assist with creating "tidy data." It includes the functions to spread and gather data to reshape it to ensure that each variable is in a column, each observation is in a row, and each value is in a cell (citation: http://tidyr.tidyverse.org/)

d. ***readr*** is the tidyverse package for importing and exporting data. "The goal of readr is to provide a fast and friendly way to read rectangular data (like csv, tsv, and fwf). It is designed to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes." (citation: http://readr.tidyverse.org/)

##1.2
a. Two functions that I've used in the ggplot2 package are ***aes()*** to supply aestheic mapping data to the graph, including setting the parameters for the coordinates, and ***geom_point()*** which plots the data specified to a scatter plot.

b. Using the dplyr package, I've leveraged the ***summarize()*** and ***filter()*** functions to select and munge data. The summarize() funciton reduces multiple values down to a single value using statistics functions like mean, count (n()), and others. The filter() function returns rows with matching conditions. It evaluates whether conditions are true (binary), such as ==, >=, and !.

c. The tidyr package in the tidyverse contains two functions that I've used for reshaping and tidying data: the ***gather()*** and the ***separate()*** functions. The gather() function takes multiple values from separate columns and combines them, and the separate() funciton can  take a single column and separate the values into separate columns which is extremely helpful for non-granular/non-atomic data.

d. For importing and exporting data using the readr package, I've used the ***read_csv()*** and ***read_delim()*** functions. Read_csv() is a more specific function than read_delim() which allows me to parse and read csv files specifically, which is very common in my line of work. Read_delim() has been helpful in other instances of files with a specific one character delimiter, like a pipe or semicolon. Again, this function also allows me to parse and read the file. 

#2 R Basics
##2.1
The only character which absolutely needs to be removed is the "1" from the variable name, since white space does not create errors in R. However, I personally would take out the extra white space in the concatenate funtion and the varible name is difficult to reproduce and contains a very confusing naming convention. 

original: My_data.name___is.too00ooLong! <- c( 1 , 2  , 3 )
should become: 
```{r}
My_data.name___is.too00ooLong <- c(1, 2  , 3)
```

##2.2 
The two errors present in this string are the capital "C" in the concatenate function, and the missing single quote following "it" in the string of values to concatonate. 

original: my_string <- C('has', 'an', 'error', 'in', 'it)
should become:
```{r}
my_string <- c('has', 'an', 'error', 'in', 'it')
```

##2.3
All of the values placed in the vector were saved as characters because the values of "3" and "4" were called as characters rather than intergers when the code was written, as indicated by the single quotes surrounding these values. As I discovered, "adding a character string to a numeric vector converts all the elements in the vector to character" (citation: https://www.statmethods.net/management/typeconversion.html).
```{r}
my_vector <- c(1, 2, '3', '4', 5)
```

I tested this by using the function is.numeric() to test my_vector, and it is indeed false, indicating that the elements in the vector are indeed characters:

```{r}
is.numeric(my_vector)
```

#Data import/export
##3.1
Importing the file into R from my desktop. I used read_delim() because I observed the raw data and notied that it was a pipe delimited "|" file. This is the code to import the file:
```{r}
file_path <- 'C:/Users/jrlang/Desktop/rail_trail.txt'
delim_data <- read_delim((file = file_path), delim = "|")
```

And to glimpse the file:
```{r}
my_data <- delim_data
glimpse(my_data)
```

##3.2
Exporting rail_trail.txt and converting it to an rds file:
```{r}
write_rds(delim_data, path = 'C:/Users/jrlang/Desktop/rail_trail.rds')
```

Reloading the newly transformed rail_trail.rds file:
Importing rds file:
```{r}
file_path <- 'C:/Users/jrlang/Desktop/rail_trail.rds'
rail_trail <- readRDS(file = file_path, refhook = NULL)
```

Glimpse the .rds file in R studio as a tibble:
```{r}
as_tibble(rail_trail)
```

#4 Visualization
##4.1
1. One significant crique of this visualization is that the circles do not appear to be directly related to sizes based on numerical values -- they appear to have been made by "eyeballing" which is misleading to the viewer. The proportions seem arbitrary, especially the looking at 22 vs 69 for the age group 45-64. This data would have been much better represented using another plotting method, such as a histogram.

2. Another issue is the colors used in this visualization, similar to the sizes of the circles, they also are arbitarily assigned. All age categories are black, women are orange, and men are purple, and there is no legend included with the graphic that may have indicated meaning. The use of colors in this graphic are necessary and does not aid the viewer in understanding.

3. Finally, a third critique of this visualization is how the data are categorized; the categories (age and gender) inconsistent in treatment of groups/circles. Presented in another format, it would make more sense to divide each age group into responses by men and women rather than separate age groups for each circle responding "yes" and "no" and just grouped by gender. Like the first issue presented, categories in this format are difficult to compare.

##4.2
See below for a recreation of the diamonds visualization:
```{r}
ggplot(data = diamonds) +
geom_boxplot(aes(x = cut, y = carat, fill = color), position = position_dodge(0), width = 5) +
labs(x = "CUT OF DIAMOND", y = "CARAT OF DIAMOND") +
coord_flip()
```

##4.3
I would change the graph so the boxplots are dodged by color, so I removed the function varwidth = TRUE. It makes the distribution of different colors more easily viewable for the reader. See below for the updated visualization:

```{r}
ggplot(data = diamonds, mapping = aes(x = cut, y = carat, fill = color)) +
  geom_boxplot() + labs(x = "CUT OF DIAMOND", y = "CARAT OF DIAMOND") + coord_flip()
```


#5 Data munging and wrangling
##5.1 
This table is not tidy, and to make it so I will separate the "cases" and "populations" values in the type column.

```{r}
spread(table2, key = type, value = count)
```

##5.2 
I first needed to create a column with the data for price per carat, which is the divided value from the price and carat columns:
```{r}
price_per_carat <- mutate(diamonds, price_per_carat = price/carat)
```

Then I needed to add the column to the diamonds tibble:
```{r}
diamonds <- price_per_carat
```

##5.3
There are 834 instances of diamonds overall which are within the parameters of over 10000 and less than a carat weight of 1.5. This is 834/53,940 of the total number of instances in the original data set, accounting for about 1.55% of the overall dataset. I used the arithmetic feature in R to add up the totals of each cut and divided that by the total number of rows (instances) in the original dataset.

I selected to output the number of each cut, and the results are surprising. I'm surprised by the skewed distribution of the number of diamonds, especially the high number of ideal cuts in this set (over half of the total). It feels like there may be missing variables in this data set which help tell the story. 

This leads into the other question, we may need to be wary of these numbers because they may only represent one retailer, one "cut" of diamond (for instance, round or emerald), or could be from a time or geographic area where diamonds were in particularly high demand. Another possibly exists that the diamonds from this set were from a dishonest distributor. There are a number of possible hidden variables in this dataset which could explain this subset.

```{r}
diamonds %>%
  filter(price > 10000, carat < 1.5) %>%
  group_by(cut) %>%
  count(cut)
```

#6 EDA
##6.1
Looking at the summary of this data, the time period represented in the txhousing dataset is between January 2000 to July 2015.

```{r}
summary(txhousing)
```

```{r}
txhousing %>%
  group_by(year, month) %>%
  arrange(desc(year))
```

##6.2
There are 46 cities represented in this dataset.
```{r}
txhousing %>%
  count(city)
```

##6.3
Houston had the highest number of sales at 8945 in July, 2015.
```{r}
txhousing %>%
  group_by(city, month, year) %>%
  arrange(desc(sales))
```

##6.4
My assumption, just looking at the table of data arranged by descending sales (and using common sense), is that it would follow for there to be a positive relationship between the number of listings and the number of sales. I am not sure how strong the relationship would be due to the number of other variables associated with this question, but I would anticipate a relationship.

To check this assumption, I plotted the sales and listing data in a scatterplot. Based on the data, there is indeed positive relationship between the number of sales and the number of listings in this dataset. However, this only includes the instances where there was data available, and there could be a number of missing listings and sales, which would not be represented in this graph.
```{r}
txhousing %>%
  group_by(sales,listings) %>%
  arrange(desc(sales)) %>%
    ggplot(mapping = aes(x = listings, y = sales)) +
  geom_point(alpha = 1) + 
  geom_smooth(se = FALSE)
```

##6.5
The table below has the proportion of missed sales for each city. There were 568 instances of missed sales overall, and the table is organzied by city. Not all cities had missed sales.
```{r}
txhousing %>% mutate(miss_sales = (is.na(sales))) %>% 
  group_by(city) %>%
            summarize(miss_sales = sum(is.na(sales)),
            n = n(),
            sum_miss_sales = sum(miss_sales, na.rm = TRUE),
            prop_sales = mean(miss_sales, na.rm = TRUE)) 
```

See the plot below for the graphical representation of the proportion of missed sales over the proportion on missed sales:
```{r}
txhousing %>% mutate(miss_sales = (is.na(sales))) %>% 
  group_by(city) %>%
            summarize(miss_sales = sum(is.na(sales)),
            n = n(),
            sum_miss_sales = sum(miss_sales, na.rm = TRUE),
            prop_sales = mean(miss_sales, na.rm = TRUE)) %>%
    ggplot(mapping = aes(x = miss_sales, y = prop_sales), alpha = city) +
  geom_point(alpha = 1) + 
  geom_smooth(se = FALSE) +
  xlab('Summary of missed sales') +
  ylab('Proportion of missing sales per city')
```


##6.6

a. The distributions of the median sales price per month do appear different when grouped by city, within a relatively confined range. See the bar graph below:
```{r}
txhousing %>%
  group_by(city, month) %>%
  filter(sales > 500) %>% summarize(med_sales = median(median, na.rm = TRUE)) %>%
  ggplot(aes(x = city, y = med_sales)) + coord_flip () +
  geom_boxplot() +
  ylab('Median sales per month larger than 500') +
  xlab('City')
```

b. Yes, I would like to investigate the highest city, Colin County as well as the city with the lowest median sales price, which I found surprising, Corpus Christi. It would be interesting to view the dates and corraborating factors which may have influenced these sales prices.

c. We would want to filter out cities and months with sales less than 500 because, due to the time contraints of the data set, would include the outliers caused by hurricanes, the economic recession and housing market decline in 2006/2007, and the crash in 2008/2009, and subsequent impact on the housing market.

To demonstrate this point, I created a graph for cities with sales less than 500 and a graph of the overall sales. It contains several outliers, and contains a much more variable overall distribution.
```{r}
txhousing %>%
  group_by(city, month) %>%
  filter(sales < 500) %>% summarize(med_sales = median(median, na.rm = TRUE)) %>%
  ggplot(aes(x = city, y = med_sales)) + coord_flip () +
  geom_bar(stat = "identity") +
  ylab('Median sales per month less than 500') +
  xlab('City')
```

Overall median sales distribution:
```{r}
txhousing %>%
  group_by(city, month) %>% 
  summarize(med_sales = median(median, na.rm = TRUE)) %>%
  ggplot(aes(x = city, y = med_sales)) + coord_flip () +
  geom_bar(stat = "identity") +
  ylab('Median sales per month') +
  xlab('City')
```